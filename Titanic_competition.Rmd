---
title: "Titanic prediction by KNN and XGBoost"
author: "ChungHao Lee"
date: "7/24/2021"
output: github_document
---

```{r}
library("tidyverse")
library("tidymodels")
library("mice")
```

```{r}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>",
  fig.path = "fig/figures/README-",
  out.width = "100%",
  message=FALSE, 
  warning=FALSE
)
```

#### Load the Titanic dataset
```{r}

df_titanic <-
  read_csv("/Users/yginger/Desktop/GitHub_repo/Data Analytics/Titanic competition/train.csv") %>% 
  rename_all(tolower) 

df_titanic_competition <- 
  read_csv("/Users/yginger/Desktop/GitHub_repo/Data Analytics/Titanic competition/test.csv") %>% 
  rename_all(tolower) 

```

#### Glimpse the dataset
```{r}
summary(df_titanic)

head(df_titanic)
```

## EDA
#### Age
```{r}
# stats of age
df_titanic %>% 
  group_by(survived) %>% 
  summarise(mean_age = mean(age, na.rm = TRUE), min_age = min(age, na.rm = TRUE), max_age = max(age, na.rm = TRUE))

# plot of age by survived or not

df_titanic %>% 
  mutate(age_group =  ifelse(age<15, "Children", ifelse(age>=15 & age <=64, "Working-age", "Elderly"))) %>%
  filter(!is.na(age_group)) %>% 
  ggplot()+
  geom_bar(mapping = aes(x = factor(age_group, level = c("Children", "Working-age", "Elderly")), fill = factor(survived)), position = "dodge")+
  labs(x = "age_group")


```
##### We can see that Children has higher survival rate compared with other age_groups. I assume it is because children has higher priority to board life raft.

#### Sex
```{r}
# Stats of sex
df_titanic %>% 
  group_by(survived) %>% 
  count(sex)

# plot of sex by survived or not
df_titanic %>% 
  ggplot()+
  geom_bar(mapping = aes(x = sex, fill = factor(survived)), position = "dodge")
```
##### We can see that female has higher survival rate compared with male. I assume it is because most passengers in titanic were from British, male from British were acting gentlemen and offer opportunity of boarding life raft to female.


#### Fare
```{r}
# Stats of fare
df_titanic %>% 
  group_by(survived) %>% 
  summarise(mean_fare = mean(fare, na.rm = TRUE), min_fare = min(fare, na.rm = TRUE), max_fare = max(fare, na.rm = TRUE))

# plot of sex by survived or not
df_titanic %>% 
  mutate(fare_category =  ifelse(fare<20, "Low", ifelse(fare>=20 & fare <=60, "Medium", "High"))) %>% 
  ggplot()+
  geom_bar(mapping = aes(x = factor(fare_category, level = c("Low", "Medium", "High")), fill = factor(survived)), position = "dodge")+
  labs(x = "fare_category")

```
##### We can see that higher fare paying has higher survival rate compared with lower fare paying. It is because higher fare can buy higher class, which had the most life raft and most resource and first priority to get saved.


#### Class
```{r}
# Stats of class
df_titanic %>% 
  group_by(survived) %>% 
  count(pclass)

# plot of class by survived or not
df_titanic %>% 
  ggplot()+
  geom_bar(mapping = aes(x = factor(pclass), fill = factor(survived)), position = "dodge")+
  labs(x = "class")
```
##### We can see that high class has higher survival rate compared with other classes. It makes sense to me because high class had the most life raft and most resource and first priority to get saved.

#### Family on board
```{r}
#Stats of family on board
df_titanic %>%
  mutate(family_on_board = sibsp + parch + 1) %>% 
  group_by(survived) %>% 
  count(family_on_board)

# mutate family on board
df_titanic <- df_titanic %>%
  mutate(family_on_board = sibsp + parch + 1)

#plot of family on board
df_titanic %>%
  group_by(family_on_board) %>% 
  mutate(survival_rate = sum(survived)/n()) %>% 
  ggplot(mapping = aes(x = factor(family_on_board), y = survival_rate, group = 1 ))+
  geom_point()+
  geom_line()
```
##### We can see that family size reaching to 3-4 has the highest survival rate. This may because in 3-4 family size, family can help each other to increase probability of survival. However, as family size increases, diminishing effect occurs, large family size got backfire, because too many members were hard to rescue in an emergency situaction.

#### Embarkation
```{r}
# Stats of class
df_titanic %>% 
  group_by(survived) %>% 
  count(embarked)

# plot of class by survived or not
df_titanic %>% 
  filter(!is.na(embarked)) %>% 
  ggplot()+
  geom_bar(mapping = aes(x = embarked, fill = factor(survived)), position = "dodge")


```
```{r}
# further explore embarkation
df_titanic %>% 
  filter(!is.na(embarked)) %>% 
  ggplot()+
  geom_col(mapping = aes(x = sex, y = fare, fill = factor(survived)), position = "dodge")+
  facet_wrap(~embarked)
```
##### It seems like C embarkation has high survival rate compared with other embarks. However, this is because passengers from C embarkation pay higher fare. 

##### With all these variables, I finally select sex, age, fare, class and family_size to build a KNN model.


## Modeling
#### KNN

##### The reason I select KNN is because the titanic dataset is not big and only got few variables. I believe KNN is a good enough model to fit the titanic prediction.
```{r}

#fix imputation 
mice_mod <- mice(df_titanic[, c("age","fare", "sex", "pclass", "embarked")], method='cart')
mice_complete <- complete(mice_mod)

df_titanic$age <- mice_complete$age
df_titanic$fare <- mice_complete$fare
df_titanic$sex <- mice_complete$sex
df_titanic$pclass <- mice_complete$pclass
df_titanic$embarked <- mice_complete$embarked


```
##### I use MICE to fill the missing values.

```{r}
df_titanic <- 
  df_titanic %>% 
  mutate(age_group =  ifelse(age<15, "1", ifelse(age>=15 & age <=64, "2", "3"))) %>% 
  mutate(fare_category =  ifelse(fare<20, "1", ifelse(fare>=20 & fare <=60, "2", "3"))) 
  
```
##### Instead of fit the model with continous variables, I group age and fare in order to decrease overfit issue in KNN.


```{r}


# Set the seed
set.seed(123)

# Converting the dependent variable into a factor

df_titanic_1 <- df_titanic %>% 
              mutate(survived = as.factor(survived))

```

```{r}
# Split the data into train and test
df_split <- initial_split(df_titanic_1)
df_train <- training(df_split)
df_test <- testing(df_split)

# Create a recipe
titanic_recipe <- 
  recipe(survived ~ age_group + fare_category + sex + pclass + family_on_board, data = df_train) %>%
  step_string2factor(all_nominal(), skip = TRUE) %>% 
  step_normalize(all_numeric(), -survived)


# Create an KNN model object
knn_model <-
  nearest_neighbor(neighbors = tune("K")) %>%
  set_engine("kknn") %>% 
  set_mode("classification")
```


```{r}
# Create a workflow
knn_workflow <-
  workflow() %>% 
  add_recipe(titanic_recipe) %>%
  add_model(knn_model)

```

```{r}
# Define the parameter to tune

knn_grid <-
  parameters(knn_workflow) %>%
  update(K = neighbors(c(1, 20))) %>% 
  grid_max_entropy(size = 10)
```

```{r}
# Find the optimal K for the KNN model
# Repeat the cross validation twice

knn_cv_results <- tune_grid(knn_workflow, resamples = vfold_cv(df_train, v = 10, repeats = 2), grid = knn_grid)
```

```{r}
# Inspect the CV results

knn_cv_results %>% collect_metrics()
```

```{r}
# Plot K vs. cross-validation accuracy

knn_cv_results %>%
  collect_metrics() %>% 
  filter(.metric == "accuracy") %>%
  ggplot(aes(x= factor(K), y= mean, group = 1)) +
  geom_line()+
  geom_point()
```

```{r}
# Select the optimal K

optimal_k <- 
  knn_cv_results %>%
  select_best("accuracy")

optimal_k
```

```{r}
# Finalize the workflow using the optimal K

knn_workflow_finalized <- finalize_workflow(knn_workflow, optimal_k)
```

```{r}
# Before making predictions, fit the data using all of the training data

fit_knn <-
  knn_workflow_finalized %>%
  fit(df_train)
```

```{r}
# Usual way of making predictions and storing the predicted classes

results_knn <- 
  predict(fit_knn, df_test) %>% 
  pluck(1) %>% 
  bind_cols(df_test, Predicted_Class = .)

results_knn
```

```{r}
# Checking the performance on the test set

conf_mat <- conf_mat(results_knn, truth = survived, estimate = Predicted_Class)

summary(conf_mat, event_level='second')
```

```{r}
# Create a ROC curve for the KNN model

results_knn_prob <- 
  predict(fit_knn, df_test, type = 'prob') %>% 
  pluck(2) %>% 
  bind_cols(df_test, Predicted_Probability = .)

roc_curve(results_knn_prob, truth = survived,
          Predicted_Probability,
          event_level = 'second') %>%
    ggplot(aes(x = 1 - specificity,
               y = sensitivity)) +
      geom_path() +
      geom_abline(lty = 3) +
      coord_equal() +
      theme_bw()

```

```{r}
# Calculate the AUC for the KNN model

roc_auc(results_knn_prob, truth = survived, Predicted_Probability, event_level = 'second')
```


#### XG Boost
```{r}
# Set the seed
set.seed(123)

# Converting the dependent variable into a factor

df_titanic_xg <- df_titanic %>% 
              mutate(survived = as.factor(survived),
                     age_group = as.numeric(age_group),
                     fare_category = as.numeric(fare_category),
                     sex = as.numeric(ifelse(sex == "male", 1, 0)),
                     pclass = as.numeric(pclass),
                     family_on_board = as.numeric(family_on_board))

# Split the data into train and test
df_split_xg <- initial_split(df_titanic_xg)
df_train_xg <- training(df_split_xg)
df_test_xg <- testing(df_split_xg)
```


```{r}
# Create a XGBoost model object

# model specification
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), min_n = tune(),
  loss_reduction = tune(),
  sample_size = tune(), mtry = tune(),
  learn_rate = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

```{r}
# set up possible values for these hyperparameters to try
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train_xg),
  learn_rate(),
  size = 10
)

xgb_grid
```
```{r}
# create recipe 
recipe_xg <- 
  recipe(survived ~ age_group + fare_category + sex + pclass + family_on_board, data = df_train_xg) 


# create workflow
xgb_wf <- workflow() %>% 
  add_recipe(recipe_xg) %>% 
  add_model(xgb_spec)
```

```{r}
# create cross-validation resamples for tuning our model.
set.seed(123)
dfa_folds <- vfold_cv(df_train_xg)
```

```{r}
doParallel::registerDoParallel()

set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = dfa_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)
```
```{r}
best_auc <- select_best(xgb_res, "roc_auc")
best_auc
```
```{r}
# Now let’s finalize our tuneable workflow with these parameter values.
final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)
```

```{r}
fitxgb <-fit(final_xgb, data = df_train_xg)
```

```{r}
# Apply model in the assessment data
results_xg <- 
  predict(fitxgb, df_test_xg, type = 'prob') %>% 
  pluck(2) %>% 
  bind_cols(df_test_xg, Predicted_Probability = .) %>% 
  mutate(predictedClass = as.factor(ifelse(Predicted_Probability > 0.5, 2, 1)))
```

```{r}
### AUC value
roc_auc(results_xg, truth = survived, Predicted_Probability, event_level = 'second')
```
```{r}
### ROC
roc_curve(results_xg, truth = survived,
          Predicted_Probability,
          event_level = 'second') %>% 
  ggplot(aes(x = 1 - specificity,
             y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()
```

## Predict
```{r}
#fix imputation 
mice_mod_competition <- mice(df_titanic_competition[, c("age","fare", "sex", "pclass", "embarked")], method='cart')
mice_complete_competition <- complete(mice_mod_competition)

df_titanic_competition$age <- mice_complete_competition$age
df_titanic_competition$fare <- mice_complete_competition$fare
df_titanic_competition$sex <- mice_complete_competition$sex
df_titanic_competition$pclass <- mice_complete_competition$pclass
df_titanic_competition$embarked <- mice_complete_competition$embarked
```


```{r}
# Making predictions and storing the predicted classes
df_titanic_competition <- df_titanic_competition %>%
  mutate(family_on_board = sibsp + parch + 1) %>% 
  mutate(age_group =  ifelse(age<15, "1", ifelse(age>=15 & age <=64, "2", "3"))) %>% 
  mutate(fare_category =  ifelse(fare<20, "1", ifelse(fare>=20 & fare <=60, "2", "3"))) 
  
```


```{r}
Prediction <- 
  predict(fit_knn, df_titanic_competition) %>% 
  pluck(1) %>% 
  bind_cols(df_titanic_competition$passengerid, Predicted_Class = .)

Prediction <- 
  Prediction %>% 
  mutate(Survived = Predicted_Class, PassengerId = ...1)
```


```{r}
### XGBoost

df_titanic_competition_xg <- 
  df_titanic_competition %>% 
  mutate(age_group = as.numeric(age_group),
                     fare_category = as.numeric(fare_category),
                     sex = as.numeric(ifelse(sex == "male", 1, 0)),
                     pclass = as.numeric(pclass),
                     family_on_board = as.numeric(family_on_board))

```


```{r}
Prediction <- 
  predict(fitxgb, df_titanic_competition_xg) %>% 
  pluck(1) %>% 
  bind_cols(df_titanic_competition_xg$passengerid, Predicted_Class = .)

Prediction_xg <- 
  Prediction %>% 
  mutate(Survived = Predicted_Class, PassengerId = ...1)
```


